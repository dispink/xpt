{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performance\n",
    "Let's check if this newly trained model (command line base) is relevant to the previous one I trained in an unprofessional way, i.e.redundant scripts that are not efficient for experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.datas.dataloader import get_dataloader\n",
    "from src.datas.transforms import InstanceNorm\n",
    "from src.eval.eval import evaluate_base, evaluate\n",
    "from src.models.mae_vit import mae_vit_base_patch16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.99951\n"
     ]
    }
   ],
   "source": [
    "dataloader = get_dataloader(annotations_file='data/pretrain/train/info.csv', input_dir='data/pretrain/train', \n",
    "                                batch_size=256, transform=InstanceNorm(), ispretrain=True)\n",
    "\n",
    "# for calculating R2 score\n",
    "mse_base= evaluate_base(dataloader['val'])\n",
    "print(f'MSE: {round(mse_base, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "MSE: 0.01065\n",
      "R2: 0.9893\n",
      "1\n",
      "MSE: 0.01048\n",
      "R2: 0.9895\n",
      "2\n",
      "MSE: 0.01121\n",
      "R2: 0.9888\n",
      "3\n",
      "MSE: 0.01091\n",
      "R2: 0.9891\n",
      "4\n",
      "MSE: 0.01058\n",
      "R2: 0.9894\n"
     ]
    }
   ],
   "source": [
    "# Our command line pre-trained model\n",
    "model = mae_vit_base_patch16().to('cuda')\n",
    "model.load_state_dict(torch.load('results/pretrain_test_20240609/model.ckpt'))\n",
    "for _ in range(5):\n",
    "    print(_)        \n",
    "    mse_model = evaluate(model, dataloader['val'])\n",
    "    print(f'MSE: {round(mse_model, 5)}')\n",
    "    print(f'R2: {round(1 - mse_model / mse_base, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "MSE: 0.01084\n",
      "R2: 0.9892\n",
      "1\n",
      "MSE: 0.01064\n",
      "R2: 0.9894\n",
      "2\n",
      "MSE: 0.01089\n",
      "R2: 0.9891\n",
      "3\n",
      "MSE: 0.01119\n",
      "R2: 0.9888\n",
      "4\n",
      "MSE: 0.0108\n",
      "R2: 0.9892\n"
     ]
    }
   ],
   "source": [
    "# Previous pre-trained model\n",
    "model_old = mae_vit_base_patch16().to('cuda')\n",
    "model_old.load_state_dict(torch.load('models/mae_vit_base_patch16_l-coslr_1e-05_20231229.pth'))\n",
    "for _ in range(5):\n",
    "    print(_)    \n",
    "    mse_model_old = evaluate(model, dataloader['val'])\n",
    "    print(f'MSE: {round(mse_model_old, 5)}')\n",
    "    print(f'R2: {round(1 - mse_model_old / mse_base, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "MSE: 0.99951\n",
      "1\n",
      "MSE: 0.99951\n",
      "2\n",
      "MSE: 0.99951\n",
      "3\n",
      "MSE: 0.99951\n",
      "4\n",
      "MSE: 0.99951\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(_)\n",
    "    mse_base= evaluate_base(dataloader['val'])\n",
    "    print(f'MSE: {round(mse_base, 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is difference between each iteration of evaluation. It happened before, but I fixed it by removing the shuffling in the validation dataloader. This time I haven't found what causes the randomness.\n",
    "\n",
    "At least there is a good news: the newly pre-trained model has relevant performance to the previous model I trained by improfessional way. The R2 are both around 0.989. The 0.996 R2 I reported based on the previous model was calculated in the raw space, which means the standardized spectra are inverse transformed to the raw space. It might be the numerical difference during the transform. And this small difference is not worthy for me to dig into."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
