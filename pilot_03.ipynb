{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After digging into the MAE's [codes](https://github.com/facebookresearch/mae/blob/main/models_mae.py), I realize the patchify codes are tailor-made for square images. Modifying the codes to fit our \"1D images\" (spectra) is not trivial, so I had to step back and use the BERT again as the backbone (the progress of `pilot_02.ipynb`) and patchify the spectra into 16*1 images in a naive way. \n",
    "\n",
    "BTW, the [notes](https://hackmd.io/lTqNcOmQQLiwzkAwVySh8Q) for MAE provide nice general understanding of the model, which is a good supplenment to the [original paper](https://arxiv.org/abs/2111.06377)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify BERT-like model\n",
    "I first want to check if the data format after patchifying is able to get through the BERT-like model. Also, the input data format is [batch_size, seq_len, embedding_dim] after torch.dataloader, which is different from format in the tutorial [seq_len, batch_size, embedding_dim]. The modifications are as follows:\n",
    "\n",
    "1. Modify the operation to fit the new input data format.\n",
    "1. Simply reshape the input data to mimic the patchification. The data shape become [batch size, spectrum length devided by patch size, patch size]. e.g., [4, 128, 16] for batch size 4, spectrum length 2048, and patch size 16.\n",
    "\n",
    "Please note that the positional encoding is patch-wise, not pixel-wise. There is no positional encoding within a patch yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        #pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        #pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        #pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            (old) x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "            (modified) x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "            embedding_dim is 16 in our case (path size: 16 channel values)\n",
    "        \"\"\"\n",
    "        #x = x + self.pe[:x.size(0)]\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    #def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "    def __init__(self, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        # no need to embed token because we are not dealing with words\n",
    "        #self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        #self.d_model = d_model\n",
    "        # no need to linearly transform the output because we aim for reconstructing the masked spectrum\n",
    "        #self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        #self.init_weights()\n",
    "\n",
    "   # these weights are not used in the current version\n",
    "    #def init_weights(self) -> None:\n",
    "    #    initrange = 0.1\n",
    "    #    self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    #    self.linear.bias.data.zero_()\n",
    "    #    self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    #def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "            the ntoken is 2048/16=128 in our case (spectrum length/patch size)\n",
    "        \"\"\"\n",
    "        #src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = src * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        #if src_mask is None:\n",
    "        #    \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "        #    Unmasked positions are filled with float(0.0).\n",
    "        #    \"\"\"\n",
    "        #    src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
    "        #output = self.transformer_encoder(src, src_mask)\n",
    "        #output = self.transformer_encoder(src)\n",
    "        #output = self.linear(output)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    # We don't need the labels and transform for now\n",
    "    def __init__(self, annotations_file, input_dir, target_dir, mask_dir):\n",
    "        \"\"\"\n",
    "        input_dir: directory with masked spe files\n",
    "        target_dir: directory with original spe files\n",
    "        mask_dir: directory with boolean mask files\n",
    "        \"\"\"\n",
    "        self.spe_info = pd.read_csv(annotations_file)\n",
    "        self.input_dir = input_dir\n",
    "        self.target_dir = target_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.spe_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_path = os.path.join(self.input_dir, self.spe_info.iloc[idx, 0])\n",
    "        target_path = os.path.join(self.target_dir, self.spe_info.iloc[idx, 0])\n",
    "        mask_path = os.path.join(self.mask_dir, self.spe_info.iloc[idx, 0])\n",
    "\n",
    "        # reshape to (128, 16)\n",
    "        input_spe = np.loadtxt(input_path, delimiter=',', dtype=int).reshape(-1, 16)\n",
    "        target_spe = np.loadtxt(target_path, delimiter=',', dtype=int).reshape(-1, 16)\n",
    "        mask = np.loadtxt(mask_path, delimiter=',', dtype=int).reshape(-1, 16)\n",
    "\n",
    "        output = {'input_spe': input_spe,\n",
    "                  'target_spe': target_spe,\n",
    "                  'mask': mask}\n",
    "  \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2],\n",
       "       [ 3,  4],\n",
       "       [ 5,  6],\n",
       "       [ 7,  8],\n",
       "       [ 9, 10]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([1,2,3,4,5,6,7,8,9,10]).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128, 16])\n",
      "{'input_spe': tensor([[[99999999, 99999999, 99999999,  ..., 99999999, 99999999, 99999999],\n",
      "         [       0,        0,        0,  ...,        0,        0, 99999999],\n",
      "         [       0,        0, 99999999,  ...,        6,        8,       16],\n",
      "         ...,\n",
      "         [99999999,        0, 99999999,  ...,        0,        0, 99999999],\n",
      "         [99999999, 99999999,        0,  ...,        0,        0, 99999999],\n",
      "         [99999999,        0, 99999999,  ..., 99999999, 99999999,        0]],\n",
      "\n",
      "        [[       0,        0,        0,  ..., 99999999,        0, 99999999],\n",
      "         [       0, 99999999,        0,  ..., 99999999, 99999999,        0],\n",
      "         [99999999,        0,        0,  ..., 99999999, 99999999,       12],\n",
      "         ...,\n",
      "         [       0,        0,        0,  ...,        0,        0,        0],\n",
      "         [       0, 99999999,        0,  ...,        0, 99999999,        0],\n",
      "         [99999999,        0, 99999999,  ...,        0, 99999999,        0]],\n",
      "\n",
      "        [[       0,        0, 99999999,  ..., 99999999,        0, 99999999],\n",
      "         [99999999,        0, 99999999,  ...,        0, 99999999, 99999999],\n",
      "         [       0,        0,        1,  ...,        7, 99999999,       15],\n",
      "         ...,\n",
      "         [       0,        0,        0,  ...,        0, 99999999, 99999999],\n",
      "         [       0,        0,        0,  ..., 99999999,        0, 99999999],\n",
      "         [       0,        0,        0,  ..., 99999999, 99999999,        0]],\n",
      "\n",
      "        [[       0,        0, 99999999,  ...,        0, 99999999, 99999999],\n",
      "         [99999999,        0,        0,  ..., 99999999,        0,        0],\n",
      "         [99999999, 99999999,        0,  ..., 99999999,       12, 99999999],\n",
      "         ...,\n",
      "         [99999999, 99999999,        0,  ...,        0, 99999999, 99999999],\n",
      "         [       0, 99999999,        0,  ..., 99999999,        0, 99999999],\n",
      "         [99999999,        0, 99999999,  ..., 99999999,        0,        0]]]), 'target_spe': tensor([[[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  6,  8, 16],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ..., 11,  9, 12],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  1,  ...,  7, 19, 15],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  1,  0,  ...,  5, 12, 13],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]]]), 'mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 1],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 0, 1,  ..., 0, 0, 1],\n",
      "         [1, 1, 0,  ..., 0, 0, 1],\n",
      "         [1, 0, 1,  ..., 1, 1, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 1, 0, 1],\n",
      "         [0, 1, 0,  ..., 1, 1, 0],\n",
      "         [1, 0, 0,  ..., 1, 1, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 1, 0,  ..., 0, 1, 0],\n",
      "         [1, 0, 1,  ..., 0, 1, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 1, 0, 1],\n",
      "         [1, 0, 1,  ..., 0, 1, 1],\n",
      "         [0, 0, 0,  ..., 0, 1, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 0, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 1, 1],\n",
      "         [1, 0, 0,  ..., 1, 0, 0],\n",
      "         [1, 1, 0,  ..., 1, 0, 1],\n",
      "         ...,\n",
      "         [1, 1, 0,  ..., 0, 1, 1],\n",
      "         [0, 1, 0,  ..., 1, 0, 1],\n",
      "         [1, 0, 1,  ..., 1, 0, 0]]])}\n"
     ]
    }
   ],
   "source": [
    "from torch import Generator\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = CustomImageDataset('data/info_20231121.csv', 'data/masked', 'data/spe', 'data/mask')\n",
    "data_train, data_test = random_split(dataset, [0.8, 0.2], generator=Generator().manual_seed(24))\n",
    "\n",
    "train_dataloader = DataLoader(data_train, batch_size=4, shuffle=True)\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch['input_spe'].size())\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 in mask is not masked. 1 in mask is masked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate an instance\n",
    "The model hyperparameters are defined below, which is identical to BERT-base (expect the head is increased to 16 to divide the emsize).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "spe_len = 2048\n",
    "patch_size = 16 \n",
    "emsize = int(spe_len/patch_size)  # embedding dimension = 128 ptaches\n",
    "d_hid = 768  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 12  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 16  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.1  # dropout probability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerModel(emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows no error! Okay, I'll modifiy the data properly and train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify MAE with our naive patchification\n",
    "Actually, modifying the MAE is not that difficult. Below codes are from `models_mae.py` in [MAE repo](https://github.com/facebookresearch/mae/blob/main/models_mae.py). Our input dataset is also modified accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# skip the delicate patch embedding\n",
    "#from timm.models.vision_transformer import PatchEmbed, Block\n",
    "from timm.models.vision_transformer import Block\n",
    "\n",
    "# it is also modified to match our naive patch embedding\n",
    "from util.pos_embed import get_1d_sincos_pos_embed\n",
    "\n",
    "\n",
    "class MaskedAutoencoderViT(nn.Module):\n",
    "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
    "    \"\"\"\n",
    "    # modify img_size to spe_size (2048)\n",
    "    # no need to have in_chans and embed_dim because we areusing naive patch embedding\n",
    "    def __init__(self, spe_size=2048, patch_size=16,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "                #self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        self.num_patches = int(spe_size/patch_size)  # 128\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size, bias=True) # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        #pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        #decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.num_patches**.5), cls_token=True)\n",
    "        decoder_pos_embed = get_1d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        # no need to initialize patch_embed because we are using naive patch embedding\n",
    "        #w = self.patch_embed.proj.weight.data\n",
    "        #torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, spes):\n",
    "        \"\"\"\n",
    "        (old)\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "\n",
    "        (modified)\n",
    "        spe: (N, spe_size)\n",
    "        x: (N, num_patches, patch_size)\n",
    "        \"\"\"\n",
    "        #p = self.patch_embed.patch_size[0]\n",
    "        #assert spes.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "        assert spes.shape[1] % self.patch_size == 0\n",
    "\n",
    "        #h = w = imgs.shape[2] // p\n",
    "        #x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = spes.reshape(shape=(spes.shape[0], self.num_patches, self.patch_size))\n",
    "        # our patch embedding is naive, no need to permute and reshape again\n",
    "        #x = torch.einsum('nchpwq->nhwpqc', x) # relevant to x.permute(0, 3, 4, 1, 2, 5)\n",
    "        #x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        (old)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "\n",
    "        (modified)\n",
    "        x: (N, num_patches, patch_size)\n",
    "        spe: (N, spe_size)\n",
    "        \"\"\"\n",
    "        #p = self.patch_size\n",
    "        #h = w = int(x.shape[1]**.5)\n",
    "        #assert h * w == x.shape[1]\n",
    "        \n",
    "        #x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        #x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        #imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        x = x.reshape(shape=(x.shape[0], self.num_patches * self.patch_size))\n",
    "        return x\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # embed patches\n",
    "        #x = self.patch_embed(x)\n",
    "        x = self.patchify(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, spes, pred, mask):\n",
    "        \"\"\"\n",
    "        (old)\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "\n",
    "        (modified)\n",
    "        spes: [N, spe_size]\n",
    "        pred: [N, num_patches, patch_size]\n",
    "        mask: [N, num_patches], 0 is keep, 1 is remove\n",
    "        \"\"\"\n",
    "        target = self.patchify(spes)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(self, spes, mask_ratio=0.75):\n",
    "        latent, mask, ids_restore = self.forward_encoder(spes, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, path_size]\n",
    "        loss = self.forward_loss(spes, pred, mask)\n",
    "        return loss, pred, mask\n",
    "    \n",
    "def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_large_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_huge_patch14_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "# set recommended archs\n",
    "mae_vit_base_patch16 = mae_vit_base_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
    "mae_vit_large_patch16 = mae_vit_large_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
    "mae_vit_huge_patch14 = mae_vit_huge_patch14_dec512d8b  # decoder: 512 dim, 8 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4163, 0.7833, 0.7865, 0.6378, 0.0169],\n",
      "        [0.7195, 0.4671, 0.3825, 0.1838, 0.6893]])\n",
      "tensor([[4, 0, 3, 1, 2],\n",
      "        [3, 2, 1, 4, 0]])\n",
      "tensor([[1, 3, 4, 2, 0],\n",
      "        [4, 2, 1, 0, 3]])\n"
     ]
    }
   ],
   "source": [
    "noise = torch.rand(2, 5) \n",
    "ids_shuffle = torch.argsort(noise, dim=1)\n",
    "ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "print(noise)\n",
    "print(ids_shuffle)\n",
    "print(ids_restore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 0, 3],\n",
       "        [3, 2, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_keep = 3\n",
    "ids_shuffle[:, :len_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedAutoencoderViT(\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (decoder_blocks): ModuleList(\n",
      "    (0-7): 8 x Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  (decoder_pred): Linear(in_features=512, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = mae_vit_base_patch16()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The version of timm is fixed in 0.4.5 as recommended by the MAE repo. Actually, MAE repo requires 0.3.2 in `main_pretrain.py`, but it has issue with the torch version. So I use 0.4.5 that mentioned in `mae_visualize.ipynb`. There will be some works regarding the version issue later since MAE doesn't provide detailed version information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, input_dir):\n",
    "        \"\"\"\n",
    "        input_dir: directory with spe files\n",
    "        \"\"\"\n",
    "        self.spe_info = pd.read_csv(annotations_file)\n",
    "        self.input_dir = input_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.spe_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_path = os.path.join(self.input_dir, self.spe_info.iloc[idx, 0])\n",
    "        spe = np.loadtxt(input_path, delimiter=',', dtype=int)  \n",
    "        return spe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spe = np.loadtxt('data/spe/1.csv', delimiter=',', dtype=int)\n",
    "spe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2048])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from torch import Generator\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = CustomImageDataset('data/info_20231121.csv', 'data/spe')\n",
    "data_train, data_test = random_split(dataset, [0.8, 0.2], generator=Generator().manual_seed(24))\n",
    "\n",
    "train_dataloader = DataLoader(data_train, batch_size=4, shuffle=True)\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch.size())\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = mae_vit_base_patch16().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _, _ = model(batch.to(device), mask_ratio=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows some errors. To debug more efficiently, I change to code in scripts. The folder structure is similiar to the MAE repo. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
