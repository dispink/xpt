{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if updating to latest version of timm (0.4.5 -> 0.9.12), which requires modifications to the code (qk_scale -> qk_norm in `models_mae_loss.py`), makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from test_loss import get_dataloader\n",
    "from util.datasets import standardize\n",
    "from models_mae_loss import mae_vit_base_patch16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_base(dataloader):\n",
    "    data = torch.empty((0, 2048))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for samples in dataloader:\n",
    "            data = torch.cat((data, samples), 0)\n",
    "        mean = data.mean()\n",
    "        loss = (data-mean)**2\n",
    "        loss = loss.mean()\n",
    "    return loss.item()\n",
    "\n",
    "def inverse_standardize(raws, pred_un):\n",
    "    mean = raws.mean(dim=1, keepdim=True)\n",
    "    std = raws.std(dim=1, keepdim=True)\n",
    "    return pred_un * std + mean\n",
    "\n",
    "def inverse_log(raws, pred_un):\n",
    "    # raws is not used, just to be consistent with other inverse functions\n",
    "    return torch.exp(pred_un) - 1\n",
    "\n",
    "def get_mse(true_values, pred_values):\n",
    "    # all in tensor\n",
    "    loss = (true_values - pred_values)**2\n",
    "    loss = loss.mean()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate_inraw(model, dataloader_raw, dataloader_norm, inverse=None, device='cuda'):\n",
    "    total_loss = 0.\n",
    "    model.eval()  # turn on evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (raws, norms) in zip(dataloader_raw, dataloader_norm):\n",
    "            raws = raws.to(device, non_blocking=True, dtype=torch.float)\n",
    "            norms = norms.to(device, non_blocking=True, dtype=torch.float)\n",
    "\n",
    "            _, pred, _ = model(norms)    # in normalized space\n",
    "            pred_un = model.unpatchify(pred)\n",
    "            if inverse:\n",
    "                pred_un = inverse(raws, pred_un)    # in raw space\n",
    "            loss = get_mse(raws, pred_un)\n",
    "            total_loss += loss\n",
    "    return total_loss / len(dataloader_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 839728.0\n",
      "R2: 0.96\n"
     ]
    }
   ],
   "source": [
    "# the base model\n",
    "dataloader_raw = get_dataloader(batch_size=64, transform=None)\n",
    "mse_base = evaluate_base(dataloader_raw['val'])\n",
    "\n",
    "# the model with standardization\n",
    "dataloader_std = get_dataloader(batch_size=64, transform=standardize)\n",
    "model = mae_vit_base_patch16().to('cuda')\n",
    "model.load_state_dict(torch.load('models/mae_vit_base_patch16_update_1e-05_20231214.pth'))\n",
    "mse_std = evaluate_inraw(model, dataloader_raw['val'], dataloader_std['val'], inverse=inverse_standardize)\n",
    "\n",
    "print(f'MSE: {round(mse_std, 1)}')\n",
    "print(f'R2: {round(1 - mse_std / mse_base, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is relevant to the optimal model trained by old version of timm, actually even slightly better. It should be the randomness of the training process. So I will use the latest version of timm to train the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
