{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow is adopted from the pytorch [tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html), which is downloaded as `transformer_tutorial.ipynb`. The code is modified to fit the task of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a ``nn.TransformerEncoder`` model following the pytroch tutorial, but with the modifications:\n",
    "1.  No embedding layer for transforming the input tokens to vectors in vocab space. \n",
    "    Our sequence is a spectrum, not text tokens.\n",
    "1.  A positional encoding layer to account for the order of the channels remains. \n",
    "    (see the next paragraph for more details). \n",
    "1.  No attention masking and decoder layer because we are not dealing with causal language modeling. \n",
    "1.  Instead of using ``CrossEntropyLoss`` as the loss function, we use ``MSELoss`` because we are dealing with regression, not classification.\n",
    "    \n",
    "The ``nn.TransformerEncoder`` consists of multiple layers of\n",
    "[nn.TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "            embedding_dim is 1 in our case\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    #def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "    def __init__(self, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        # no need to embed token because we are not dealing with words\n",
    "        #self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        #self.d_model = d_model\n",
    "        # no need to linearly transform the output because we aim for reconstructing the masked spectrum\n",
    "        #self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        #self.init_weights()\n",
    "\n",
    "   # these weights are not used in the current version\n",
    "    #def init_weights(self) -> None:\n",
    "    #    initrange = 0.1\n",
    "    #    self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    #    self.linear.bias.data.zero_()\n",
    "    #    self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    #def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "            the ntoken is 1 because we are not dealing with words\n",
    "        \"\"\"\n",
    "        #src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = src * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        #if src_mask is None:\n",
    "        #    \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "        #    Unmasked positions are filled with float(0.0).\n",
    "        #    \"\"\"\n",
    "        #    src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
    "        #output = self.transformer_encoder(src, src_mask)\n",
    "        #output = self.transformer_encoder(src)\n",
    "        #output = self.linear(output)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    # We don't need the labels and transform for now\n",
    "    def __init__(self, annotations_file, input_dir, target_dir, mask_dir):\n",
    "        \"\"\"\n",
    "        input_dir: directory with masked spe files\n",
    "        target_dir: directory with original spe files\n",
    "        mask_dir: directory with boolean mask files\n",
    "        \"\"\"\n",
    "        self.spe_info = pd.read_csv(annotations_file)\n",
    "        self.input_dir = input_dir\n",
    "        self.target_dir = target_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.spe_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_path = os.path.join(self.input_dir, self.spe_info.iloc[idx, 0])\n",
    "        target_path = os.path.join(self.target_dir, self.spe_info.iloc[idx, 0])\n",
    "        mask_path = os.path.join(self.mask_dir, self.spe_info.iloc[idx, 0])\n",
    "\n",
    "        input_spe = np.loadtxt(input_path, delimiter=',', dtype=int)\n",
    "        target_spe = np.loadtxt(target_path, delimiter=',', dtype=int)\n",
    "        mask = np.loadtxt(mask_path, delimiter=',', dtype=int)\n",
    "\n",
    "        output = {'input_spe': input_spe,\n",
    "                  'target_spe': target_spe,\n",
    "                  'mask': mask}\n",
    "  \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2048])\n",
      "{'input_spe': tensor([[99999999,        0, 99999999,  ...,        0,        0,        0],\n",
      "        [       0,        0, 99999999,  ...,        0,        0,        0],\n",
      "        [       0,        0,        0,  ...,        0, 99999999, 99999999],\n",
      "        [       0, 99999999,        0,  ...,        0, 99999999,        0]]), 'target_spe': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'mask': tensor([[1, 0, 1,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 1, 1],\n",
      "        [0, 1, 0,  ..., 0, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from torch import Generator\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = CustomImageDataset('data/info_20231121.csv', 'data/masked', 'data/spe', 'data/mask')\n",
    "data_train, data_test = random_split(dataset, [0.8, 0.2], generator=Generator().manual_seed(24))\n",
    "\n",
    "train_dataloader = DataLoader(data_train, batch_size=4, shuffle=True)\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch['input_spe'].size())\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 in mask is not masked. 1 in mask is masked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate an instance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model hyperparameters are defined below, which is identical to BERT-base. The ``vocab`` size is\n",
    "equal to the length of the vocab object.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/xpt/pilot_02.ipynb Cell 12\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f61736c65652f787074222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f61736c65652f7870742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2B192.168.1.66/workspaces/xpt/pilot_02.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m dropout \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m  \u001b[39m# dropout probability\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f61736c65652f787074222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f61736c65652f7870742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2B192.168.1.66/workspaces/xpt/pilot_02.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f61736c65652f787074222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f61736c65652f7870742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2B192.168.1.66/workspaces/xpt/pilot_02.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m model \u001b[39m=\u001b[39m TransformerModel(d_model, nhead, d_hid, nlayers, dropout)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m/workspaces/xpt/pilot_02.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f61736c65652f787074222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f61736c65652f7870742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2B192.168.1.66/workspaces/xpt/pilot_02.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTransformer\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f61736c65652f787074222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f61736c65652f7870742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2B192.168.1.66/workspaces/xpt/pilot_02.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder \u001b[39m=\u001b[39m PositionalEncoding(d_model, dropout)\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f61736c65652f787074222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f61736c65652f7870742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2B192.168.1.66/workspaces/xpt/pilot_02.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m encoder_layers \u001b[39m=\u001b[39m TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f61736c65652f787074222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f61736c65652f7870742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2B192.168.1.66/workspaces/xpt/pilot_02.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_encoder \u001b[39m=\u001b[39m TransformerEncoder(encoder_layers, nlayers)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/transformer.py:553\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.__init__\u001b[0;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, bias, device, dtype)\u001b[0m\n\u001b[1;32m    551\u001b[0m factory_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: device, \u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m: dtype}\n\u001b[1;32m    552\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 553\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn \u001b[39m=\u001b[39m MultiheadAttention(d_model, nhead, dropout\u001b[39m=\u001b[39;49mdropout,\n\u001b[1;32m    554\u001b[0m                                     bias\u001b[39m=\u001b[39;49mbias, batch_first\u001b[39m=\u001b[39;49mbatch_first,\n\u001b[1;32m    555\u001b[0m                                     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs)\n\u001b[1;32m    556\u001b[0m \u001b[39m# Implementation of Feedforward model\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear1 \u001b[39m=\u001b[39m Linear(d_model, dim_feedforward, bias\u001b[39m=\u001b[39mbias, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/activation.py:991\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[0;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39m=\u001b[39m batch_first\n\u001b[1;32m    990\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim \u001b[39m=\u001b[39m embed_dim \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m num_heads\n\u001b[0;32m--> 991\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim \u001b[39m*\u001b[39m num_heads \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39m\"\u001b[39m\u001b[39membed_dim must be divisible by num_heads\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qkv_same_embed_dim:\n\u001b[1;32m    994\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((embed_dim, embed_dim), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "#ntokens = len(vocab)  # size of vocabulary\n",
    "spe_len = 2048  # spectrum length\n",
    "d_hid = 768  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 12  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 8  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.1  # dropout probability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerModel(spe_len, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 2048])\n"
     ]
    }
   ],
   "source": [
    "output = model(batch['input_spe'].to(device))\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 4, 1])\n",
      "torch.Size([2048, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "d_model = 1\n",
    "dropout = 0.1\n",
    "pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "src = batch['input_spe'].view(2048, -1, 1)\n",
    "# modify src to have the shape [seq_len, batch_size, embedding_dim]\n",
    "\n",
    "\n",
    "src = src * math.sqrt(d_model)\n",
    "print(src.size())\n",
    "src = pos_encoder(src)\n",
    "print(src.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short note\n",
    "\n",
    "There are two errors that I have encountered. The first one is `embed_dim` (sequence length), which needs to be divisible by the number of heads. The planned length is 2048, which can not be divided by the BERT-base default heads, 12. It's not as critical as the second one...\n",
    "\n",
    "The second one is the `d_model`, the number of expected features in the input. It is relevent to the `vocab_size` in BERT. Every tocken in BERT is represented by a vector of `d_model` features, so the expected input shape is [sequence length, vocab size] (regardless the batch size). The masking is applied on the sequence-wise, i.e. masking tokens. In my case, the input data is a 1D spectrum, which has the shape [spectrum length] only.  The spectrum consists 2048 integers recording the counts across the wavelength range. Since my goal is training a model that is able to reconstruct the masked parts of a spectrum, I should treat the spectrum as a sequence in BERT perspective. However, this makes the input feature's shape become 1. This is, hence, rejected by the transformer architecture. The attention mechanism can’t calculate [1,1] vector.\n",
    "\n",
    "And Hsuan-Tien recommend me to use the architecture of ViT and treat my spectrum as an 1D image. The pixel values is relevent to the counts of the spectrum. They face the same problem as I do, but they solve it by cutting the pixels into patches. I think I can try this approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
